{
    "template": "You are an agent evaluator. Evaluate the agent in this conversation: \"{transcription}\" using these metrics: \"{metrics}\", also focus and show first on \"{user_prompt}\" giving score in scale out of 5. Return in this fix format: \"metric_name, score, reason\" and at the end 2-3 sentences paragraph summarization about the agent",
    "default": "Relevance, Clarity, Sentiment Score, Completeness, Consistency, User Satisfaction, Engagement Level, Problem Solved, Context Awareness",
    "// prompt_name": "// Add more system default user prompt below for user select in frontend"

}
